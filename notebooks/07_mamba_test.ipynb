{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5016cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "from models.mamba import MambaConfig, MambaSSM, MambaModelForHourlySeries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "df = pd.read_csv(\"../data/merged_data.csv\", parse_dates=[\"datetime\"])\n",
    "df = df.sort_values(\"datetime\")\n",
    "\n",
    "df[\"hour\"] = df[\"datetime\"].dt.hour\n",
    "df[\"is_daytime\"] = ((df[\"hour\"] >= 7) & (df[\"hour\"] < 19)).astype(int)\n",
    "\n",
    "df = df.drop(columns=[\"hour\"])\n",
    "\n",
    "df[\"sin_hour\"] = np.sin(2 * np.pi * df[\"datetime\"].dt.hour / 24)\n",
    "df[\"cos_hour\"] = np.cos(2 * np.pi * df[\"datetime\"].dt.hour / 24)\n",
    "df[\"price_lag_1h\"] = df[\"price_EUR_MWh\"].shift(1)\n",
    "\n",
    "df = df.dropna()\n",
    "\n",
    "# features = [\n",
    "#     \"temperature_2m\", \"wind_speed_180m\", \"wind_speed_120m\", \"direct_radiation\",\n",
    "#     \"quantity_biomass\", \"quantity_fossil_gas\", \"quantity_fossil_hard_coal\",\n",
    "#     \"quantity_hydro_run_of_river\", \"quantity_nuclear\", \"quantity_solar\",\n",
    "#     \"quantity_waste\", \"quantity_wind_offshore\", \"quantity_wind_onshore\",\n",
    "#     \"quantity_other\", \"quantity_MW\"\n",
    "# ]\n",
    "# target = \"price_EUR_MWh\"\n",
    "\n",
    "features = [\n",
    "    \"temperature_2m\", \"wind_speed_180m\", \"wind_speed_120m\", \"direct_radiation\", \"quantity_solar\",\n",
    "    \"quantity_other\", \"price_lag_1h\", \"is_daytime\", \"sin_hour\", \"cos_hour\", \"quantity_MW\"\n",
    "]\n",
    "target = \"price_EUR_MWh\"\n",
    "\n",
    "# Splitting indices before scaling\n",
    "val_days = 30\n",
    "val_size = val_days * 24\n",
    "\n",
    "train_df = df.iloc[:-val_size]\n",
    "val_df = df.iloc[-val_size:]\n",
    "\n",
    "# Fit scalers ONLY on training data\n",
    "scaler_x = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "scaler_x.fit(train_df[features])\n",
    "scaler_y.fit(train_df[[target]])\n",
    "\n",
    "# Transform entire dataset safely\n",
    "X = scaler_x.transform(df[features])\n",
    "y = scaler_y.transform(df[[target]])\n",
    "\n",
    "# scaler_x = StandardScaler()\n",
    "# scaler_y = StandardScaler()\n",
    "\n",
    "# # X = scaler_x.fit_transform(df[features])\n",
    "# # y = scaler_y.fit_transform(df[[target]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc4ebd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for time series data with fixed sequence length.\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, seq_len):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X) - self.seq_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            self.X[idx:idx+self.seq_len],\n",
    "            self.y[idx+self.seq_len]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "061ca3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HourAheadWrapper(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super().__init__()\n",
    "        self.base = base_model  \n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        seq = self.base(x)  # [B, 24]\n",
    "        return seq[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edb52249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into training and validation sets\n",
    "# Assuming the data is hourly and we want to validate on the last 30 days\n",
    "sequence_length = 24 # 24 hours = 1 day\n",
    "val_days = 30\n",
    "val_size = val_days * 24\n",
    "\n",
    "train_X = X[:-val_size]\n",
    "train_y = y[:-val_size]\n",
    "val_X = X[-val_size - sequence_length:]\n",
    "\n",
    "val_y = y[-val_size - sequence_length:]\n",
    "\n",
    "train_dataset = TimeSeriesDataset(train_X, train_y, sequence_length)\n",
    "val_dataset = TimeSeriesDataset(val_X, val_y, sequence_length)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0650be5a",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     28\u001b[39m x, t = x.cuda(), t.cuda()\n\u001b[32m     29\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m preds = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# print(f\"preds shape: {preds.shape}, t shape: {t.shape}, t.squeeze(-1) shape: {t.squeeze(-1).shape}\")\u001b[39;00m\n\u001b[32m     32\u001b[39m loss = criterion(preds, t.squeeze(-\u001b[32m1\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Uni\\Zurich\\Semester 1\\Advanced Machine Learning\\project\\aml2025-group-15-main\\aml2025-group-15\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Uni\\Zurich\\Semester 1\\Advanced Machine Learning\\project\\aml2025-group-15-main\\aml2025-group-15\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mHourAheadWrapper.forward\u001b[39m\u001b[34m(self, x, **kwargs)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, **kwargs):\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     seq = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [B, 24]\u001b[39;00m\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m seq[:, -\u001b[32m1\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Uni\\Zurich\\Semester 1\\Advanced Machine Learning\\project\\aml2025-group-15-main\\aml2025-group-15\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Uni\\Zurich\\Semester 1\\Advanced Machine Learning\\project\\aml2025-group-15-main\\aml2025-group-15\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Uni\\Zurich\\Semester 1\\Advanced Machine Learning\\project\\aml2025-group-15-main\\aml2025-group-15\\notebooks\\../src\\models\\mamba.py:159\u001b[39m, in \u001b[36mMambaModelForHourlySeries.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    150\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    151\u001b[39m \u001b[33;03mArgs\u001b[39;00m\n\u001b[32m    152\u001b[39m \u001b[33;03m----\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    156\u001b[39m \u001b[33;03mTensor of shape (batch, 24)\u001b[39;00m\n\u001b[32m    157\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    158\u001b[39m x = \u001b[38;5;28mself\u001b[39m.input_proj(x)   \u001b[38;5;66;03m# (B, 24, hidden)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmamba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m        \u001b[38;5;66;03m# (B, 24, hidden)\u001b[39;00m\n\u001b[32m    160\u001b[39m x = \u001b[38;5;28mself\u001b[39m.output_proj(x)  \u001b[38;5;66;03m# (B, 24, 1)\u001b[39;00m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x.squeeze(-\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Uni\\Zurich\\Semester 1\\Advanced Machine Learning\\project\\aml2025-group-15-main\\aml2025-group-15\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Uni\\Zurich\\Semester 1\\Advanced Machine Learning\\project\\aml2025-group-15-main\\aml2025-group-15\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Uni\\Zurich\\Semester 1\\Advanced Machine Learning\\project\\aml2025-group-15-main\\aml2025-group-15\\notebooks\\../src\\models\\mamba.py:120\u001b[39m, in \u001b[36mMamba.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    118\u001b[39m conv_out = \u001b[38;5;28mself\u001b[39m.conv(k.transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m))[..., :sequence_length]\n\u001b[32m    119\u001b[39m conv_out = torch.nn.functional.silu(conv_out)\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m ssm_out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mssm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconv_out\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    122\u001b[39m output = ssm_out * torch.nn.functional.silu(q)\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.out_proj(output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Uni\\Zurich\\Semester 1\\Advanced Machine Learning\\project\\aml2025-group-15-main\\aml2025-group-15\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Uni\\Zurich\\Semester 1\\Advanced Machine Learning\\project\\aml2025-group-15-main\\aml2025-group-15\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Uni\\Zurich\\Semester 1\\Advanced Machine Learning\\project\\aml2025-group-15-main\\aml2025-group-15\\notebooks\\../src\\models\\mamba.py:76\u001b[39m, in \u001b[36mMambaSSM.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m     73\u001b[39m current_C = C[:, i, :]\n\u001b[32m     75\u001b[39m \u001b[38;5;66;03m# Update the current state\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m Ax = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbhn, bhn -> bhn\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_A_discrete\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     77\u001b[39m Bu = torch.einsum(\u001b[33m'\u001b[39m\u001b[33mbhn, bh -> bhn\u001b[39m\u001b[33m'\u001b[39m, current_B_discrete, current_input)\n\u001b[32m     78\u001b[39m x = Ax + Bu\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Uni\\Zurich\\Semester 1\\Advanced Machine Learning\\project\\aml2025-group-15-main\\aml2025-group-15\\.venv\\Lib\\site-packages\\torch\\functional.py:402\u001b[39m, in \u001b[36meinsum\u001b[39m\u001b[34m(*args)\u001b[39m\n\u001b[32m    397\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m einsum(equation, *_operands)\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(operands) <= \u001b[32m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_einsum.enabled:\n\u001b[32m    400\u001b[39m     \u001b[38;5;66;03m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[32m    401\u001b[39m     \u001b[38;5;66;03m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m402\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[43m.\u001b[49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    404\u001b[39m path = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    405\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m opt_einsum.is_available():\n",
      "\u001b[31mRuntimeError\u001b[39m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "# Model Config\n",
    "\n",
    "config = MambaConfig(    \n",
    "    state_size   = 64,\n",
    "    hidden_size  = 32,\n",
    "    expansion_factor = 2,\n",
    "    dt_rank      = 3,\n",
    "    conv_kernel_size = 3\n",
    ")\n",
    "\n",
    "# Model, optimizer, and loss function\n",
    "n_epochs = 30\n",
    "\n",
    "model = HourAheadWrapper(MambaModelForHourlySeries(config, in_features=11)).cuda()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-3, weight_decay=1e-4)\n",
    "criterion = nn.L1Loss().cuda()\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=n_epochs)\n",
    "\n",
    "\n",
    "\n",
    "# Training the model\n",
    "\n",
    "model.train()\n",
    "for epoch in range(n_epochs):\n",
    "    epoch_loss = 0\n",
    "    for x, t in train_loader:\n",
    "        # print(f\"Current learning rate: {scheduler.get_last_lr()}\")\n",
    "        x, t = x.cuda(), t.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(x)\n",
    "        # print(f\"preds shape: {preds.shape}, t shape: {t.shape}, t.squeeze(-1) shape: {t.squeeze(-1).shape}\")\n",
    "        loss = criterion(preds, t.squeeze(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x, t in val_loader:\n",
    "            x, t = x.cuda(), t.cuda()\n",
    "            preds = model(x)\n",
    "            loss = criterion(preds, t.squeeze(-1))\n",
    "            val_loss += loss.item()\n",
    "    model.train()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {epoch_loss/len(train_loader):.4f}, Val Loss: {val_loss/len(val_loader):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
